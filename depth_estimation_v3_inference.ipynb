{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "isGpuEnabled": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Depth Estimation —Å Depth Anything V3 + Segmentation\n",
        "\n",
        "**–û–±–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å Depth Anything V3!**\n",
        "\n",
        "–≠—Ç–æ—Ç notebook:\n",
        "1. **–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Depth Anything V3 (DA3NESTED)** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ camera intrinsics!\n",
        "2. **Mask R-CNN** –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫—É–±–æ–≤ –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–æ–≤\n",
        "3. **–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è** –¥–æ –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –≤ –º–µ—Ç—Ä–∞—Ö\n",
        "4. **–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–≤—è–∑–∏** –º–µ–∂–¥—É –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–∞–º–∏ –∏ –∫—É–±–∞–º–∏\n",
        "5. **–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç** —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å depth map, confidence –∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è–º–∏\n",
        "\n",
        "## üéØ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ V3:\n",
        "\n",
        "- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç camera intrinsics** (–Ω–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∏–∑ Unity!)\n",
        "- ‚úÖ **Depth —Å—Ä–∞–∑—É –≤ –º–µ—Ç—Ä–∞—Ö** (–Ω–µ –Ω—É–∂–Ω–∞ —Ñ–æ—Ä–º—É–ª–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏)\n",
        "- ‚úÖ **Confidence map** –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ–Ω–∞–¥—ë–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π\n",
        "- ‚úÖ **–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å** –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å V2\n",
        "- ‚úÖ **Multi-view support** (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Depth Anything V3\n",
        "!pip install xformers torch>=2 torchvision\n",
        "!pip install git+https://github.com/ByteDance-Seed/Depth-Anything-3.git\n",
        "\n",
        "# Segmentation\n",
        "!pip install pycocotools\n",
        "!pip install opencv-python-headless\n",
        "!pip install matplotlib\n",
        "!pip install scipy\n",
        "!pip install pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "from depth_anything_3.api import DepthAnything3\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- –õ–û–ì–ò–ö–ê –ó–ê–ì–†–£–ó–ö–ò –î–ê–¢–ê–°–ï–¢–ê ---\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "REPO_URL = \"https://github.com/SergKurchev/strawberry_peduncle_segmentation.git\"\n",
        "REPO_NAME = \"strawberry_peduncle_segmentation\"\n",
        "KAGGLE_PATH = \"/kaggle/input/strawberry-peduncle-segmentation\"\n",
        "\n",
        "# 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–∞ Kaggle (–µ—Å–ª–∏ –∑–∞–ø—É—Å–∫ —Ç–∞–º)\n",
        "if os.path.exists(KAGGLE_PATH):\n",
        "    DATASET_PATH = KAGGLE_PATH\n",
        "    print(f\"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ Kaggle Input: {DATASET_PATH}\")\n",
        "else:\n",
        "    # 2. –ï—Å–ª–∏ –Ω–∞ Kaggle –Ω–µ—Ç, –∫–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏–∑ GitHub\n",
        "    if not os.path.exists(REPO_NAME):\n",
        "        print(f\"üöÄ –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏–∑ GitHub: {REPO_URL}...\")\n",
        "        subprocess.run([\"git\", \"clone\", REPO_URL])\n",
        "    else:\n",
        "        print(f\"‚úÖ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π {REPO_NAME} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.\")\n",
        "    \n",
        "    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –≤–Ω—É—Ç—Ä–∏ —Ä–µ–ø–æ\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (—Å –≤–ª–æ–∂–µ–Ω–Ω–æ–π –ø–∞–ø–∫–æ–π –∏ –±–µ–∑)\n",
        "    opt1 = os.path.join(REPO_NAME, \"strawberry_peduncle_segmentation\", \"dataset\")\n",
        "    opt2 = os.path.join(REPO_NAME, \"dataset\")\n",
        "    \n",
        "    if os.path.exists(opt1):\n",
        "        DATASET_PATH = opt1\n",
        "    elif os.path.exists(opt2):\n",
        "        DATASET_PATH = opt2\n",
        "    else:\n",
        "        DATASET_PATH = REPO_NAME\n",
        "        print(f\"‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –ü–∞–ø–∫–∞ 'dataset' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä–µ–Ω—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.\")\n",
        "\n",
        "print(f\"üìç –ò—Ç–æ–≥–æ–≤—ã–π –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É: {DATASET_PATH}\")\n",
        "\n",
        "IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\n",
        "MASKS_PATH = os.path.join(DATASET_PATH, \"masks\")\n",
        "ANNOTATIONS_PATH = os.path.join(DATASET_PATH, \"annotations.json\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
        "if not os.path.exists(IMAGES_PATH):\n",
        "    print(f\"‚ùå –í–ù–ò–ú–ê–ù–ò–ï: –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {IMAGES_PATH}\")\n",
        "    print(f\"   –ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ GitHub, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π.\")\n",
        "    print(f\"   –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ Kaggle, –¥–æ–±–∞–≤—å—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –≤ Input.\")\n",
        "\n",
        "# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "NUM_CLASSES = 3  # background + red_cube + green_parallelepiped\n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 30\n",
        "LEARNING_RATE = 0.005\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "CATEGORIES = {\n",
        "    0: \"background\",\n",
        "    1: \"red_cube\",\n",
        "    2: \"green_parallelepiped\"\n",
        "}\n",
        "\n",
        "# –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
        "TRAIN_RATIO = 0.8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Depth Anything V3\n",
        "print(f\"Loading Depth Anything V3: {DEPTH_MODEL_NAME}\")\n",
        "\n",
        "depth_model = DepthAnything3.from_pretrained(DEPTH_MODEL_NAME)\n",
        "depth_model = depth_model.to(DEVICE)\n",
        "\n",
        "print(\"‚úÖ Depth Anything V3 loaded\")\n",
        "print(f\"   Model: {DEPTH_MODEL_NAME}\")\n",
        "print(f\"   Features: Auto camera intrinsics, metric depth, confidence map\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ Mask R-CNN –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
        "def get_segmentation_model(num_classes):\n",
        "    model = maskrcnn_resnet50_fpn(weights=None)\n",
        "    \n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    \n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
        "    \n",
        "    return model\n",
        "\n",
        "segmentation_model = get_segmentation_model(num_classes=3)\n",
        "segmentation_model.load_state_dict(torch.load(SEGMENTATION_MODEL_PATH, map_location='cpu'))\n",
        "segmentation_model = segmentation_model.to(DEVICE).eval()\n",
        "\n",
        "print(\"‚úÖ Mask R-CNN loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_depth_v3(image_rgb):\n",
        "    \"\"\"\n",
        "    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç depth map –≤ –º–µ—Ç—Ä–∞—Ö –∏—Å–ø–æ–ª—å–∑—É—è Depth Anything V3.\n",
        "    \n",
        "    Args:\n",
        "        image_rgb: RGB –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (numpy array –∏–ª–∏ PIL Image)\n",
        "    \n",
        "    Returns:\n",
        "        result: dict —Å depth, confidence, intrinsics\n",
        "    \"\"\"\n",
        "    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ PIL –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
        "    if isinstance(image_rgb, np.ndarray):\n",
        "        image_pil = Image.fromarray(image_rgb)\n",
        "    else:\n",
        "        image_pil = image_rgb\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # V3 –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
        "        prediction = depth_model.inference([image_pil])\n",
        "    \n",
        "    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "    depth = prediction.depth[0]  # (H, W) –≤ –ú–ï–¢–†–ê–•!\n",
        "    conf = prediction.conf[0] if hasattr(prediction, 'conf') else None  # (H, W) confidence\n",
        "    intrinsics = prediction.intrinsics[0] if hasattr(prediction, 'intrinsics') else None  # (3, 3)\n",
        "    \n",
        "    return {\n",
        "        'depth': depth,\n",
        "        'confidence': conf,\n",
        "        'intrinsics': intrinsics,\n",
        "        'focal_length': intrinsics[0, 0] if intrinsics is not None else None\n",
        "    }\n",
        "\n",
        "\n",
        "def predict_segmentation(image_rgb, score_threshold=0.5):\n",
        "    \"\"\"\n",
        "    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –æ–±—ä–µ–∫—Ç–æ–≤.\n",
        "    \"\"\"\n",
        "    if isinstance(image_rgb, np.ndarray):\n",
        "        image_tensor = torch.as_tensor(image_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
        "    else:\n",
        "        image_tensor = torch.as_tensor(np.array(image_rgb), dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        prediction = segmentation_model([image_tensor.to(DEVICE)])[0]\n",
        "    \n",
        "    keep = prediction['scores'] >= score_threshold\n",
        "    filtered = {\n",
        "        'boxes': prediction['boxes'][keep].cpu(),\n",
        "        'labels': prediction['labels'][keep].cpu(),\n",
        "        'scores': prediction['scores'][keep].cpu(),\n",
        "        'masks': prediction['masks'][keep].cpu()\n",
        "    }\n",
        "    \n",
        "    return filtered\n",
        "\n",
        "\n",
        "def predict_associations(boxes, labels, scores):\n",
        "    \"\"\"\n",
        "    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–∞–º–∏ –∏ –∫—É–±–∞–º–∏.\n",
        "    \"\"\"\n",
        "    boxes_np = boxes.numpy() if isinstance(boxes, torch.Tensor) else boxes\n",
        "    labels_np = labels.numpy() if isinstance(labels, torch.Tensor) else labels\n",
        "    \n",
        "    cube_indices = np.where(labels_np == 1)[0]\n",
        "    para_indices = np.where(labels_np == 2)[0]\n",
        "    \n",
        "    associations = {}\n",
        "    \n",
        "    for para_idx in para_indices:\n",
        "        para_box = boxes_np[para_idx]\n",
        "        para_center_x = (para_box[0] + para_box[2]) / 2\n",
        "        para_bottom = para_box[3]\n",
        "        \n",
        "        min_dist = float('inf')\n",
        "        best_cube_idx = None\n",
        "        \n",
        "        for cube_idx in cube_indices:\n",
        "            cube_box = boxes_np[cube_idx]\n",
        "            cube_center_x = (cube_box[0] + cube_box[2]) / 2\n",
        "            cube_top = cube_box[1]\n",
        "            \n",
        "            vertical_dist = abs(para_bottom - cube_top)\n",
        "            horizontal_dist = abs(para_center_x - cube_center_x)\n",
        "            \n",
        "            dist = np.sqrt(vertical_dist**2 + horizontal_dist**2)\n",
        "            \n",
        "            if cube_box[0] <= para_center_x <= cube_box[2]:\n",
        "                dist *= 0.5\n",
        "            \n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                best_cube_idx = cube_idx\n",
        "        \n",
        "        associations[int(para_idx)] = int(best_cube_idx) if best_cube_idx is not None else None\n",
        "    \n",
        "    return associations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_object_distance(depth_map, confidence_map, mask, boxes, idx, conf_threshold=0.3):\n",
        "    \"\"\"\n",
        "    –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –æ–±—ä–µ–∫—Ç–∞ —Å —É—á—ë—Ç–æ–º confidence map.\n",
        "    \n",
        "    Args:\n",
        "        depth_map: (H, W) depth –≤ –º–µ—Ç—Ä–∞—Ö\n",
        "        confidence_map: (H, W) confidence (0-1) –∏–ª–∏ None\n",
        "        mask: –º–∞—Å–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤\n",
        "        boxes: bounding boxes\n",
        "        idx: –∏–Ω–¥–µ–∫—Å –æ–±—ä–µ–∫—Ç–∞\n",
        "        conf_threshold: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è confidence\n",
        "    \n",
        "    Returns:\n",
        "        distance_info: dict —Å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è–º–∏ –∏ confidence\n",
        "    \"\"\"\n",
        "    if isinstance(mask, torch.Tensor):\n",
        "        obj_mask = mask[idx, 0].numpy() > 0.5\n",
        "    else:\n",
        "        obj_mask = mask > 0.5\n",
        "    \n",
        "    # –ü—Ä–∏–º–µ–Ω—è–µ–º confidence —Ñ–∏–ª—å—Ç—Ä –µ—Å–ª–∏ –µ—Å—Ç—å\n",
        "    if confidence_map is not None:\n",
        "        high_conf_mask = confidence_map >= conf_threshold\n",
        "        obj_mask = obj_mask & high_conf_mask\n",
        "    \n",
        "    object_depths = depth_map[obj_mask]\n",
        "    \n",
        "    if len(object_depths) == 0:\n",
        "        return {\n",
        "            'min': 0.0,\n",
        "            'max': 0.0,\n",
        "            'mean': 0.0,\n",
        "            'median': 0.0,\n",
        "            'center': 0.0,\n",
        "            'confidence': 0.0,\n",
        "            'valid_pixels': 0\n",
        "        }\n",
        "    \n",
        "    # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ü–µ–Ω—Ç—Ä–∞ bbox\n",
        "    box = boxes[idx].numpy() if isinstance(boxes[idx], torch.Tensor) else boxes[idx]\n",
        "    center_y = int((box[1] + box[3]) / 2)\n",
        "    center_x = int((box[0] + box[2]) / 2)\n",
        "    center_depth = depth_map[center_y, center_x]\n",
        "    \n",
        "    # –°—Ä–µ–¥–Ω—è—è confidence –¥–ª—è –æ–±—ä–µ–∫—Ç–∞\n",
        "    avg_conf = 1.0\n",
        "    if confidence_map is not None:\n",
        "        obj_mask_full = mask[idx, 0].numpy() > 0.5 if isinstance(mask, torch.Tensor) else mask > 0.5\n",
        "        avg_conf = float(confidence_map[obj_mask_full].mean())\n",
        "    \n",
        "    return {\n",
        "        'min': float(np.min(object_depths)),\n",
        "        'max': float(np.max(object_depths)),\n",
        "        'mean': float(np.mean(object_depths)),\n",
        "        'median': float(np.median(object_depths)),\n",
        "        'center': float(center_depth),\n",
        "        'confidence': avg_conf,\n",
        "        'valid_pixels': len(object_depths)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. –ü–æ–ª–Ω—ã–π inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def full_inference(image_path, visualize=True):\n",
        "    \"\"\"\n",
        "    –ü–æ–ª–Ω—ã–π pipeline —Å Depth Anything V3.\n",
        "    \"\"\"\n",
        "    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "    image_rgb = np.array(Image.open(image_path).convert('RGB'))\n",
        "    \n",
        "    print(f\"üì∏ Processing: {os.path.basename(image_path)}\")\n",
        "    \n",
        "    # 2. Depth estimation —Å V3\n",
        "    print(\"   üîç Estimating depth with V3...\")\n",
        "    depth_result = predict_depth_v3(image_rgb)\n",
        "    \n",
        "    depth_map = depth_result['depth']\n",
        "    confidence_map = depth_result['confidence']\n",
        "    intrinsics = depth_result['intrinsics']\n",
        "    focal_length = depth_result['focal_length']\n",
        "    \n",
        "    print(f\"      Depth range: {depth_map.min():.2f}m - {depth_map.max():.2f}m\")\n",
        "    if focal_length is not None:\n",
        "        print(f\"      Estimated focal length: {focal_length:.2f}px (expected: {EXPECTED_FOCAL_LENGTH:.2f}px)\")\n",
        "        error_pct = abs(focal_length - EXPECTED_FOCAL_LENGTH) / EXPECTED_FOCAL_LENGTH * 100\n",
        "        print(f\"      Focal length error: {error_pct:.1f}%\")\n",
        "    \n",
        "    # 3. Segmentation\n",
        "    print(\"   üéØ Segmenting objects...\")\n",
        "    segmentation = predict_segmentation(image_rgb)\n",
        "    \n",
        "    # 4. Associations\n",
        "    print(\"   üîó Predicting associations...\")\n",
        "    associations = predict_associations(\n",
        "        segmentation['boxes'],\n",
        "        segmentation['labels'],\n",
        "        segmentation['scores']\n",
        "    )\n",
        "    \n",
        "    # 5. Calculate distances\n",
        "    print(\"   üìè Calculating distances...\")\n",
        "    distances = []\n",
        "    for i in range(len(segmentation['labels'])):\n",
        "        dist_info = calculate_object_distance(\n",
        "            depth_map,\n",
        "            confidence_map,\n",
        "            segmentation['masks'],\n",
        "            segmentation['boxes'],\n",
        "            i,\n",
        "            conf_threshold=CONFIDENCE_THRESHOLD\n",
        "        )\n",
        "        distances.append(dist_info)\n",
        "    \n",
        "    # 6. Summary\n",
        "    n_cubes = (segmentation['labels'] == 1).sum().item()\n",
        "    n_paras = (segmentation['labels'] == 2).sum().item()\n",
        "    \n",
        "    print(f\"\\n   ‚úÖ Results:\")\n",
        "    print(f\"      Cubes: {n_cubes}\")\n",
        "    print(f\"      Parallelepipeds: {n_paras}\")\n",
        "    print(f\"      Associations: {len(associations)}\")\n",
        "    \n",
        "    # 7. Visualization\n",
        "    if visualize:\n",
        "        visualize_results(\n",
        "            image_rgb,\n",
        "            depth_map,\n",
        "            confidence_map,\n",
        "            segmentation,\n",
        "            associations,\n",
        "            distances,\n",
        "            intrinsics\n",
        "        )\n",
        "    \n",
        "    return {\n",
        "        'image': image_rgb,\n",
        "        'depth_map': depth_map,\n",
        "        'confidence_map': confidence_map,\n",
        "        'intrinsics': intrinsics,\n",
        "        'segmentation': segmentation,\n",
        "        'associations': associations,\n",
        "        'distances': distances\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_results(image, depth_map, confidence_map, segmentation, associations, distances, intrinsics):\n",
        "    \"\"\"\n",
        "    –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∫–ª—é—á–∞—è confidence map.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    boxes = segmentation['boxes'].numpy()\n",
        "    labels = segmentation['labels'].numpy()\n",
        "    scores = segmentation['scores'].numpy()\n",
        "    masks = segmentation['masks'].numpy()\n",
        "    \n",
        "    # 1. Original RGB\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    ax1.imshow(image)\n",
        "    ax1.set_title('Original Image', fontsize=14, fontweight='bold')\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    # 2. Depth Map\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    depth_vis = ax2.imshow(depth_map, cmap='turbo')\n",
        "    ax2.set_title(f'Depth Map (V3)\\nRange: {depth_map.min():.2f}m - {depth_map.max():.2f}m', \n",
        "                  fontsize=14, fontweight='bold')\n",
        "    ax2.axis('off')\n",
        "    plt.colorbar(depth_vis, ax=ax2, fraction=0.046, pad=0.04, label='Depth (m)')\n",
        "    \n",
        "    # 3. Confidence Map\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    if confidence_map is not None:\n",
        "        conf_vis = ax3.imshow(confidence_map, cmap='viridis', vmin=0, vmax=1)\n",
        "        ax3.set_title(f'Confidence Map\\nMean: {confidence_map.mean():.3f}', \n",
        "                      fontsize=14, fontweight='bold')\n",
        "        plt.colorbar(conf_vis, ax=ax3, fraction=0.046, pad=0.04, label='Confidence')\n",
        "    else:\n",
        "        ax3.text(0.5, 0.5, 'No confidence map', ha='center', va='center')\n",
        "        ax3.set_title('Confidence Map', fontsize=14, fontweight='bold')\n",
        "    ax3.axis('off')\n",
        "    \n",
        "    # 4. Segmentation with distances\n",
        "    ax4 = fig.add_subplot(gs[1, :])\n",
        "    ax4.imshow(image)\n",
        "    \n",
        "    colors = {1: 'red', 2: 'green'}\n",
        "    \n",
        "    for i in range(len(labels)):\n",
        "        box = boxes[i]\n",
        "        label = labels[i]\n",
        "        dist = distances[i]\n",
        "        color = colors.get(label, 'blue')\n",
        "        \n",
        "        # Draw bbox\n",
        "        from matplotlib.patches import Rectangle\n",
        "        rect = Rectangle(\n",
        "            (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
        "            fill=False, edgecolor=color, linewidth=2\n",
        "        )\n",
        "        ax4.add_patch(rect)\n",
        "        \n",
        "        # Draw label with distance and confidence\n",
        "        text = f\"{CATEGORIES[label]}\\n{dist['center']:.2f}m\\nconf: {dist['confidence']:.2f}\"\n",
        "        ax4.text(\n",
        "            box[0], box[1] - 10,\n",
        "            text,\n",
        "            color='white',\n",
        "            fontsize=9,\n",
        "            bbox=dict(boxstyle='round', facecolor=color, alpha=0.8)\n",
        "        )\n",
        "    \n",
        "    ax4.set_title('Segmentation + Distances + Confidence', fontsize=14, fontweight='bold')\n",
        "    ax4.axis('off')\n",
        "    \n",
        "    # 5. Masks with associations\n",
        "    ax5 = fig.add_subplot(gs[2, :])\n",
        "    ax5.imshow(image)\n",
        "    \n",
        "    # Draw masks\n",
        "    combined_mask = np.zeros((*image.shape[:2], 3))\n",
        "    for i in range(len(labels)):\n",
        "        mask = masks[i, 0] > 0.5\n",
        "        label = labels[i]\n",
        "        \n",
        "        if label == 1:\n",
        "            combined_mask[mask] = [1, 0, 0]\n",
        "        elif label == 2:\n",
        "            combined_mask[mask] = [0, 1, 0]\n",
        "    \n",
        "    ax5.imshow(combined_mask, alpha=0.5)\n",
        "    \n",
        "    # Draw association lines\n",
        "    for para_idx, cube_idx in associations.items():\n",
        "        if cube_idx is not None:\n",
        "            para_box = boxes[para_idx]\n",
        "            cube_box = boxes[cube_idx]\n",
        "            \n",
        "            para_center = [(para_box[0] + para_box[2]) / 2, (para_box[1] + para_box[3]) / 2]\n",
        "            cube_center = [(cube_box[0] + cube_box[2]) / 2, (cube_box[1] + cube_box[3]) / 2]\n",
        "            \n",
        "            ax5.plot(\n",
        "                [para_center[0], cube_center[0]],\n",
        "                [para_center[1], cube_center[1]],\n",
        "                'y-', linewidth=2\n",
        "            )\n",
        "            ax5.plot(*para_center, 'yo', markersize=8)\n",
        "            ax5.plot(*cube_center, 'yo', markersize=8)\n",
        "    \n",
        "    ax5.set_title('Masks + Associations', fontsize=14, fontweight='bold')\n",
        "    ax5.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed info\n",
        "    print(\"\\nüìä Detailed Information:\")\n",
        "    print(\"=\" * 90)\n",
        "    \n",
        "    if intrinsics is not None:\n",
        "        print(f\"\\nüì∑ Estimated Camera Intrinsics:\")\n",
        "        print(f\"   Focal Length X: {intrinsics[0, 0]:.2f}px\")\n",
        "        print(f\"   Focal Length Y: {intrinsics[1, 1]:.2f}px\")\n",
        "        print(f\"   Principal Point: ({intrinsics[0, 2]:.2f}, {intrinsics[1, 2]:.2f})\")\n",
        "    \n",
        "    print(f\"\\nüìè Object Distances:\")\n",
        "    for i in range(len(labels)):\n",
        "        label_name = CATEGORIES[labels[i]]\n",
        "        dist = distances[i]\n",
        "        parent_info = \"\"\n",
        "        \n",
        "        if i in associations:\n",
        "            parent_idx = associations[i]\n",
        "            if parent_idx is not None:\n",
        "                parent_dist = distances[parent_idx]\n",
        "                parent_info = f\" ‚Üí Parent cube at {parent_dist['center']:.2f}m\"\n",
        "        \n",
        "        print(f\"[{i}] {label_name}:\")\n",
        "        print(f\"    Center: {dist['center']:.3f}m | Mean: {dist['mean']:.3f}m | \"\n",
        "              f\"Range: {dist['min']:.3f}m - {dist['max']:.3f}m\")\n",
        "        print(f\"    Confidence: {dist['confidence']:.3f} | Valid pixels: {dist['valid_pixels']}{parent_info}\")\n",
        "    print(\"=\" * 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. –ó–∞–ø—É—Å–∫ inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "image_files = sorted(glob.glob(os.path.join(IMAGES_PATH, \"*.png\")))[:5]\n",
        "\n",
        "print(f\"Found {len(image_files)} images for inference\\n\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for img_path in image_files:\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    results = full_inference(img_path, visualize=True)\n",
        "    all_results.append(results)\n",
        "    print(\"=\"*90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Unity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
        "all_cube_distances = []\n",
        "all_para_distances = []\n",
        "all_focal_lengths = []\n",
        "\n",
        "for result in all_results:\n",
        "    labels = result['segmentation']['labels'].numpy()\n",
        "    distances = result['distances']\n",
        "    \n",
        "    for i, label in enumerate(labels):\n",
        "        if label == 1:\n",
        "            all_cube_distances.append(distances[i]['center'])\n",
        "        elif label == 2:\n",
        "            all_para_distances.append(distances[i]['center'])\n",
        "    \n",
        "    if result['intrinsics'] is not None:\n",
        "        all_focal_lengths.append(result['intrinsics'][0, 0])\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Distance distributions\n",
        "axes[0].hist(all_cube_distances, bins=20, color='red', alpha=0.7, edgecolor='black')\n",
        "axes[0].set_title('Distance Distribution: Red Cubes', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Distance (m)')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].hist(all_para_distances, bins=20, color='green', alpha=0.7, edgecolor='black')\n",
        "axes[1].set_title('Distance Distribution: Green Parallelepipeds', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Distance (m)')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Focal length comparison\n",
        "if all_focal_lengths:\n",
        "    axes[2].hist(all_focal_lengths, bins=15, color='blue', alpha=0.7, edgecolor='black')\n",
        "    axes[2].axvline(EXPECTED_FOCAL_LENGTH, color='red', linestyle='--', linewidth=2, \n",
        "                    label=f'Unity Expected: {EXPECTED_FOCAL_LENGTH:.1f}px')\n",
        "    axes[2].set_title('Estimated Focal Length Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[2].set_xlabel('Focal Length (pixels)')\n",
        "    axes[2].set_ylabel('Count')\n",
        "    axes[2].legend()\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìà Overall Statistics:\")\n",
        "print(f\"\\n   Cubes: {len(all_cube_distances)} detected\")\n",
        "print(f\"      Mean distance: {np.mean(all_cube_distances):.3f}m\")\n",
        "print(f\"      Std: {np.std(all_cube_distances):.3f}m\")\n",
        "print(f\"      Range: {np.min(all_cube_distances):.3f}m - {np.max(all_cube_distances):.3f}m\")\n",
        "\n",
        "print(f\"\\n   Parallelepipeds: {len(all_para_distances)} detected\")\n",
        "print(f\"      Mean distance: {np.mean(all_para_distances):.3f}m\")\n",
        "print(f\"      Std: {np.std(all_para_distances):.3f}m\")\n",
        "print(f\"      Range: {np.min(all_para_distances):.3f}m - {np.max(all_para_distances):.3f}m\")\n",
        "\n",
        "if all_focal_lengths:\n",
        "    print(f\"\\n   üì∑ Focal Length Estimation:\")\n",
        "    print(f\"      Mean estimated: {np.mean(all_focal_lengths):.2f}px\")\n",
        "    print(f\"      Unity expected: {EXPECTED_FOCAL_LENGTH:.2f}px\")\n",
        "    print(f\"      Error: {abs(np.mean(all_focal_lengths) - EXPECTED_FOCAL_LENGTH):.2f}px \"\n",
        "          f\"({abs(np.mean(all_focal_lengths) - EXPECTED_FOCAL_LENGTH) / EXPECTED_FOCAL_LENGTH * 100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_results = []\n",
        "\n",
        "for i, (img_path, result) in enumerate(zip(image_files, all_results)):\n",
        "    labels = result['segmentation']['labels'].numpy()\n",
        "    boxes = result['segmentation']['boxes'].numpy()\n",
        "    scores = result['segmentation']['scores'].numpy()\n",
        "    distances = result['distances']\n",
        "    associations = result['associations']\n",
        "    intrinsics = result['intrinsics']\n",
        "    \n",
        "    objects = []\n",
        "    for j in range(len(labels)):\n",
        "        obj = {\n",
        "            'id': j,\n",
        "            'category': CATEGORIES[labels[j]],\n",
        "            'bbox': boxes[j].tolist(),\n",
        "            'score': float(scores[j]),\n",
        "            'distance': {\n",
        "                'center': distances[j]['center'],\n",
        "                'mean': distances[j]['mean'],\n",
        "                'min': distances[j]['min'],\n",
        "                'max': distances[j]['max'],\n",
        "                'confidence': distances[j]['confidence']\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        if j in associations:\n",
        "            obj['parent_id'] = associations[j]\n",
        "        \n",
        "        objects.append(obj)\n",
        "    \n",
        "    result_dict = {\n",
        "        'image': os.path.basename(img_path),\n",
        "        'depth_range': {\n",
        "            'min': float(result['depth_map'].min()),\n",
        "            'max': float(result['depth_map'].max()),\n",
        "            'mean': float(result['depth_map'].mean())\n",
        "        },\n",
        "        'objects': objects\n",
        "    }\n",
        "    \n",
        "    if intrinsics is not None:\n",
        "        result_dict['camera_intrinsics'] = {\n",
        "            'focal_length_x': float(intrinsics[0, 0]),\n",
        "            'focal_length_y': float(intrinsics[1, 1]),\n",
        "            'principal_point_x': float(intrinsics[0, 2]),\n",
        "            'principal_point_y': float(intrinsics[1, 2])\n",
        "        }\n",
        "    \n",
        "    output_results.append(result_dict)\n",
        "\n",
        "with open('depth_v3_segmentation_results.json', 'w') as f:\n",
        "    json.dump(output_results, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Results saved to depth_v3_segmentation_results.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù –†–µ–∑—é–º–µ: Depth Anything V3\n",
        "\n",
        "### –ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç notebook:\n",
        "\n",
        "1. **Depth Estimation —Å V3**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç camera intrinsics –∏ depth –≤ –º–µ—Ç—Ä–∞—Ö\n",
        "2. **Confidence Map**: –§–∏–ª—å—Ç—Ä—É–µ—Ç –Ω–µ–Ω–∞–¥—ë–∂–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ depth\n",
        "3. **Segmentation**: Mask R-CNN –¥–ª—è –∫—É–±–æ–≤ –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–æ–≤\n",
        "4. **Associations**: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏\n",
        "5. **Distance Calculation**: –¢–æ—á–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è —Å —É—á—ë—Ç–æ–º confidence\n",
        "\n",
        "### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ V3 –Ω–∞–¥ V2:\n",
        "\n",
        "- ‚úÖ **–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ focal length** - –Ω–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∏–∑ Unity!\n",
        "- ‚úÖ **Depth —Å—Ä–∞–∑—É –≤ –º–µ—Ç—Ä–∞—Ö** (–¥–ª—è Nested –º–æ–¥–µ–ª–µ–π)\n",
        "- ‚úÖ **Confidence map** –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n",
        "- ‚úÖ **–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å** –Ω–∞ indoor —Å—Ü–µ–Ω–∞—Ö\n",
        "- ‚úÖ **Multi-view support** (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "\n",
        "### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:\n",
        "\n",
        "- –†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞ (–Ω–∞–≤–∏–≥–∞—Ü–∏—è, –∑–∞—Ö–≤–∞—Ç –æ–±—ä–µ–∫—Ç–æ–≤)\n",
        "- AR/VR (—Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤)\n",
        "- –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π)\n",
        "- 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (—Å multi-view)\n",
        "- –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ (–∏–∑–º–µ—Ä–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤)"
      ]
    }
  ]
}