{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫—É–±–æ–≤ –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–æ–≤ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Å–≤—è–∑–µ–π\n",
        "\n",
        "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å Mask R-CNN –¥–ª—è:\n",
        "1. **Instance Segmentation** –∫—Ä–∞—Å–Ω—ã—Ö –∫—É–±–æ–≤ (3√ó3√ó3 —Å–º) –∏ –∑–µ–ª—ë–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–æ–≤ (0.3√ó0.3√ó2 —Å–º)\n",
        "2. **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π** –∫–∞–∫–æ–π –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –∫–∞–∫–æ–º—É –∫—É–±—É —á–µ—Ä–µ–∑ `parent_id`\n",
        "\n",
        "–î–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ COCO —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—è–º–∏:\n",
        "- `instance_id` ‚Äî —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –æ–±—ä–µ–∫—Ç–∞ –≤ —Å—Ü–µ–Ω–µ\n",
        "- `parent_id` ‚Äî –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–æ–≤ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ `instance_id` —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫—É–±–∞\n",
        "- `visibility_ratio` ‚Äî % –≤–∏–¥–∏–º–æ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–∞ (—É—á–∏—Ç—ã–≤–∞–µ—Ç –æ–∫–∫–ª—é–∑–∏–∏)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (2.10.0)\n",
            "Requirement already satisfied: torchvision in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (0.25.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (2025.10.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torchvision) (2.3.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torchvision) (12.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.3 -> 26.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pycocotools\n",
            "  Downloading pycocotools-2.0.11-cp312-abi3-win_amd64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\nevergonnagiveyouup\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from pycocotools) (2.3.4)\n",
            "Downloading pycocotools-2.0.11-cp312-abi3-win_amd64.whl (77 kB)\n",
            "Installing collected packages: pycocotools\n",
            "Successfully installed pycocotools-2.0.11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.3 -> 26.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install pycocotools\n",
        "!pip install opencv-python-headless\n",
        "!pip install albumentations\n",
        "!pip install matplotlib\n",
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- –õ–û–ì–ò–ö–ê –ó–ê–ì–†–£–ó–ö–ò –î–ê–¢–ê–°–ï–¢–ê ---\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "REPO_URL = \"https://github.com/SergKurchev/strawberry_peduncle_segmentation.git\"\n",
        "REPO_NAME = \"strawberry_peduncle_segmentation\"\n",
        "KAGGLE_PATH = \"/kaggle/input/strawberry-peduncle-segmentation\"\n",
        "\n",
        "# 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–∞ Kaggle (–µ—Å–ª–∏ –∑–∞–ø—É—Å–∫ —Ç–∞–º)\n",
        "if os.path.exists(KAGGLE_PATH):\n",
        "    DATASET_PATH = KAGGLE_PATH\n",
        "    print(f\"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ Kaggle Input: {DATASET_PATH}\")\n",
        "else:\n",
        "    # 2. –ï—Å–ª–∏ –Ω–∞ Kaggle –Ω–µ—Ç, –∫–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏–∑ GitHub\n",
        "    if not os.path.exists(REPO_NAME):\n",
        "        print(f\"üöÄ –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏–∑ GitHub: {REPO_URL}...\")\n",
        "        subprocess.run([\"git\", \"clone\", REPO_URL])\n",
        "    else:\n",
        "        print(f\"‚úÖ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π {REPO_NAME} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.\")\n",
        "    \n",
        "    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –≤–Ω—É—Ç—Ä–∏ —Ä–µ–ø–æ\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (—Å –≤–ª–æ–∂–µ–Ω–Ω–æ–π –ø–∞–ø–∫–æ–π –∏ –±–µ–∑)\n",
        "    opt1 = os.path.join(REPO_NAME, \"strawberry_peduncle_segmentation\", \"dataset\")\n",
        "    opt2 = os.path.join(REPO_NAME, \"dataset\")\n",
        "    \n",
        "    if os.path.exists(opt1):\n",
        "        DATASET_PATH = opt1\n",
        "    elif os.path.exists(opt2):\n",
        "        DATASET_PATH = opt2\n",
        "    else:\n",
        "        DATASET_PATH = REPO_NAME\n",
        "        print(f\"‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –ü–∞–ø–∫–∞ 'dataset' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä–µ–Ω—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.\")\n",
        "\n",
        "print(f\"üìç –ò—Ç–æ–≥–æ–≤—ã–π –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É: {DATASET_PATH}\")\n",
        "\n",
        "IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\n",
        "MASKS_PATH = os.path.join(DATASET_PATH, \"masks\")\n",
        "ANNOTATIONS_PATH = os.path.join(DATASET_PATH, \"annotations.json\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
        "if not os.path.exists(IMAGES_PATH):\n",
        "    print(f\"‚ùå –í–ù–ò–ú–ê–ù–ò–ï: –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {IMAGES_PATH}\")\n",
        "    print(f\"   –ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ GitHub, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π.\")\n",
        "    print(f\"   –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ Kaggle, –¥–æ–±–∞–≤—å—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –≤ Input.\")\n",
        "\n",
        "# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "NUM_CLASSES = 3  # background + red_cube + green_parallelepiped\n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 30\n",
        "LEARNING_RATE = 0.005\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "CATEGORIES = {\n",
        "    0: \"background\",\n",
        "    1: \"red_cube\",\n",
        "    2: \"green_parallelepiped\"\n",
        "}\n",
        "\n",
        "# –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
        "TRAIN_RATIO = 0.8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π\n",
        "with open(ANNOTATIONS_PATH, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:\")\n",
        "print(f\"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(data['images'])}\")\n",
        "print(f\"   –ê–Ω–Ω–æ—Ç–∞—Ü–∏–π: {len(data['annotations'])}\")\n",
        "print(f\"   –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {len(data['categories'])}\")\n",
        "\n",
        "# –ü–æ–¥—Å—á—ë—Ç –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
        "category_counts = defaultdict(int)\n",
        "for ann in data['annotations']:\n",
        "    category_counts[ann['category_id']] += 1\n",
        "\n",
        "print(f\"\\nüì¶ –û–±—ä–µ–∫—Ç–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\")\n",
        "for cat in data['categories']:\n",
        "    print(f\"   {cat['name']}: {category_counts[cat['id']]}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤—è–∑–µ–π parent_id\n",
        "cubes = [a for a in data['annotations'] if a['category_id'] == 1]\n",
        "paras = [a for a in data['annotations'] if a['category_id'] == 2]\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥ instance_id –∫—É–±–∞\n",
        "cube_instance_ids = set()\n",
        "for ann in data['annotations']:\n",
        "    if ann['category_id'] == 1:\n",
        "        cube_instance_ids.add((ann['image_id'], ann['instance_id']))\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º parent_id –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–æ–≤\n",
        "valid_parents = 0\n",
        "for ann in data['annotations']:\n",
        "    if ann['category_id'] == 2 and ann['parent_id'] > 0:\n",
        "        if (ann['image_id'], ann['parent_id']) in cube_instance_ids:\n",
        "            valid_parents += 1\n",
        "\n",
        "print(f\"\\nüîó –°–≤—è–∑–∏ parent_id:\")\n",
        "print(f\"   –ü–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–æ–≤ —Å –≤–∞–ª–∏–¥–Ω—ã–º parent_id: {valid_parents}/{len(paras)}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ visibility_ratio –µ—Å–ª–∏ –µ—Å—Ç—å\n",
        "if 'visibility_ratio' in data['annotations'][0]:\n",
        "    vis_ratios = [a.get('visibility_ratio', 1.0) for a in data['annotations']]\n",
        "    print(f\"\\nüëÅÔ∏è Visibility ratio:\")\n",
        "    print(f\"   Min: {min(vis_ratios):.2f}, Max: {max(vis_ratios):.2f}, Avg: {np.mean(vis_ratios):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (V4: ID Collision Fix)\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "sample_image_id = data['images'][0]['id']\n",
        "sample_image_info = data['images'][0]\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –º–∞—Å–∫—É\n",
        "img = Image.open(os.path.join(IMAGES_PATH, sample_image_info['file_name']))\n",
        "mask = Image.open(os.path.join(MASKS_PATH, sample_image_info['file_name']))\n",
        "\n",
        "# –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "sample_anns = [a for a in data['annotations'] if a['image_id'] == sample_image_id]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# RGB –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
        "axes[0].imshow(img)\n",
        "axes[0].set_title('RGB Image')\n",
        "\n",
        "# –ú–∞—Å–∫–∞ (—É—Å–∏–ª–µ–Ω–Ω–∞—è)\n",
        "mask_np = np.array(mask)\n",
        "enhanced_mask = np.clip(mask_np.astype(np.float32) * 30, 0, 255).astype(np.uint8)\n",
        "axes[1].imshow(enhanced_mask)\n",
        "axes[1].set_title('Segmentation Mask (enhanced)')\n",
        "\n",
        "# RGB —Å bbox (Calculate from Mask!)\n",
        "axes[2].imshow(img)\n",
        "\n",
        "mask_img_np = np.array(mask.convert(\"RGB\"))\n",
        "print(f\"\\nüìù Objects found in mask:\")\n",
        "\n",
        "# Shared Logic Helper\n",
        "def get_cleaned_obj(mask_img_np, seg_color):\n",
        "    obj_mask = np.all(mask_img_np == seg_color, axis=2).astype(np.uint8)\n",
        "    if obj_mask.sum() == 0: return None\n",
        "    num_labels, labels_im, stats, centroids = cv2.connectedComponentsWithStats(obj_mask, connectivity=8)\n",
        "    if num_labels <= 1: return None\n",
        "    largest_label = 1\n",
        "    max_area = stats[1, cv2.CC_STAT_AREA]\n",
        "    for i in range(2, num_labels):\n",
        "        if stats[i, cv2.CC_STAT_AREA] > max_area:\n",
        "            max_area = stats[i, cv2.CC_STAT_AREA]\n",
        "            largest_label = i\n",
        "    if max_area < 50: return None # 50px Threshold\n",
        "    \n",
        "    # Get BBox & Center\n",
        "    obj_mask = (labels_im == largest_label).astype(np.uint8)\n",
        "    pos = np.where(obj_mask)\n",
        "    xmin, xmax = np.min(pos[1]), np.max(pos[1])\n",
        "    ymin, ymax = np.min(pos[0]), np.max(pos[0])\n",
        "    bbox = (xmin, ymin, xmax-xmin, ymax-ymin)\n",
        "    center = centroids[largest_label]\n",
        "    return bbox, center\n",
        "\n",
        "cube_objects = {}\n",
        "all_objects = {}\n",
        "\n",
        "for ann in sample_anns:\n",
        "    res = get_cleaned_obj(mask_img_np, ann['segmentation_color'])\n",
        "    if res:\n",
        "        all_objects[ann['instance_id']] = (res, ann['category_id'])\n",
        "        if ann['category_id'] == 1:\n",
        "            cube_objects[ann['instance_id']] = res\n",
        "\n",
        "for ann in sample_anns:\n",
        "    if ann['instance_id'] not in all_objects: \n",
        "        continue\n",
        "    \n",
        "    (bbox, center), cat_id = all_objects[ann['instance_id']]\n",
        "    (x, y, w, h) = bbox\n",
        "    \n",
        "    color = 'red' if cat_id == 1 else 'green'\n",
        "    rect = plt.Rectangle((x, y), w, h, fill=False, edgecolor=color, linewidth=2)\n",
        "    axes[2].add_patch(rect)\n",
        "\n",
        "    # Draw associations (Targeting Cubes Only)\n",
        "    if cat_id == 2 and ann['parent_id'] > 0:\n",
        "        # Look in CUBE map\n",
        "        if ann['parent_id'] in cube_objects:\n",
        "             px, py = cube_objects[ann['parent_id']][1]\n",
        "             cx, cy = center\n",
        "             \n",
        "             # CHECK DISTANCE\n",
        "             dist = ((px-cx)**2 + (py-cy)**2)**0.5\n",
        "             if dist < 200:\n",
        "                 axes[2].plot([cx, px], [cy, py], 'y-', linewidth=2)\n",
        "\n",
        "axes[2].set_title('Bounding Boxes with Associations (SANITIZED)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìù –ê–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏: {len(sample_anns)}\")\n",
        "for ann in sample_anns:\n",
        "    cat_name = CATEGORIES[ann['category_id']]\n",
        "    parent_info = f\" -> parent: {ann['parent_id']}\" if ann['parent_id'] > 0 else \"\"\n",
        "    print(f\"   [{ann['instance_id']}] {cat_name}{parent_info}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Dataset –∫–ª–∞—Å—Å"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CubeParallelepipedDataset(Dataset):\n",
        "    \"\"\"Dataset –¥–ª—è –∫—É–±–æ–≤ –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–≤—è–∑—è—Ö.\"\"\"\n",
        "    \n",
        "    def __init__(self, images_path, masks_path, annotations_path, transforms=None):\n",
        "        self.images_path = images_path\n",
        "        self.masks_path = masks_path\n",
        "        self.transforms = transforms\n",
        "        \n",
        "        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π\n",
        "        with open(annotations_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        self.images_info = {img['id']: img for img in data['images']}\n",
        "        \n",
        "        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º\n",
        "        self.annotations_by_image = defaultdict(list)\n",
        "        for ann in data['annotations']:\n",
        "            self.annotations_by_image[ann['image_id']].append(ann)\n",
        "        \n",
        "        self.image_ids = list(self.images_info.keys())\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.image_ids[idx]\n",
        "        image_info = self.images_info[image_id]\n",
        "        \n",
        "        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "        img_path = os.path.join(self.images_path, image_info['file_name'])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = np.array(image)\n",
        "        \n",
        "        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–∞—Å–∫–∏\n",
        "        mask_path = os.path.join(self.masks_path, image_info['file_name'])\n",
        "        mask_image = np.array(Image.open(mask_path).convert(\"RGB\"))\n",
        "        \n",
        "        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è —ç—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "        annotations = self.annotations_by_image[image_id]\n",
        "        \n",
        "        boxes = []\n",
        "        labels = []\n",
        "        masks = []\n",
        "        instance_ids = []\n",
        "        parent_ids = []\n",
        "        \n",
        "        # Maps for sanitation: Only Cubes (Cat 1) can be parents\n",
        "        cube_centroids = {}\n",
        "        processed_objects = []\n",
        "        \n",
        "        # Helper function \n",
        "        def process_mask(ann, mask_img):\n",
        "            seg_color = ann['segmentation_color']\n",
        "            obj_mask = np.all(mask_img == seg_color, axis=2).astype(np.uint8)\n",
        "            if obj_mask.sum() == 0: return None, None, None\n",
        "            \n",
        "            import cv2\n",
        "            num_labels, labels_im, stats, centroids = cv2.connectedComponentsWithStats(obj_mask, connectivity=8)\n",
        "            if num_labels <= 1: return None, None, None\n",
        "            \n",
        "            largest_label = 1\n",
        "            max_area = stats[1, cv2.CC_STAT_AREA]\n",
        "            for i in range(2, num_labels):\n",
        "                if stats[i, cv2.CC_STAT_AREA] > max_area:\n",
        "                    max_area = stats[i, cv2.CC_STAT_AREA]\n",
        "                    largest_label = i\n",
        "            \n",
        "            # Filter small noise (increased threshold)\n",
        "            if max_area < 50: return None, None, None\n",
        "            \n",
        "            cleaned_mask = (labels_im == largest_label).astype(np.uint8)\n",
        "            \n",
        "            pos = np.where(cleaned_mask)\n",
        "            xmin, xmax = np.min(pos[1]), np.max(pos[1])\n",
        "            ymin, ymax = np.min(pos[0]), np.max(pos[0])\n",
        "            if xmax <= xmin: xmax = xmin + 1\n",
        "            if ymax <= ymin: ymax = ymin + 1\n",
        "            \n",
        "            center = centroids[largest_label]\n",
        "            \n",
        "            return cleaned_mask, [xmin, ymin, xmax, ymax], center\n",
        "\n",
        "        # Pass 1: Extract all objects\n",
        "        for ann in annotations:\n",
        "             msk, box, center = process_mask(ann, mask_image)\n",
        "             if msk is not None:\n",
        "                 processed_objects.append((ann, msk, box, center))\n",
        "                 # Only register CUBES as valid parents\n",
        "                 if ann['category_id'] == 1:\n",
        "                     cube_centroids[ann['instance_id']] = center\n",
        "        \n",
        "        # Pass 2: Commit targets with Sanitation\n",
        "        for (ann, msk, box, center) in processed_objects:\n",
        "            pid = ann['parent_id']\n",
        "            \n",
        "            if pid > 0:\n",
        "                # 1. Check if Parent ID exists in CUBES map\n",
        "                if pid not in cube_centroids:\n",
        "                    pid = 0\n",
        "                else:\n",
        "                    # 2. Check Distance\n",
        "                    pcx, pcy = cube_centroids[pid]\n",
        "                    ccx, ccy = center\n",
        "                    dist = ((pcx-ccx)**2 + (pcy-ccy)**2)**0.5\n",
        "                    if dist > 200:\n",
        "                        pid = 0 # Too far\n",
        "            \n",
        "            boxes.append(box)\n",
        "            labels.append(ann['category_id'])\n",
        "            instance_ids.append(ann['instance_id'])\n",
        "            parent_ids.append(pid)\n",
        "            masks.append(msk)\n",
        "        \n",
        "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
        "        num_objs = len(boxes)\n",
        "        \n",
        "        if num_objs == 0:\n",
        "            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –æ–±—ä–µ–∫—Ç–æ–≤\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "            masks = torch.zeros((0, image.shape[0], image.shape[1]), dtype=torch.uint8)\n",
        "            instance_ids = torch.zeros((0,), dtype=torch.int64)\n",
        "            parent_ids = torch.zeros((0,), dtype=torch.int64)\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "            masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
        "            instance_ids = torch.as_tensor(instance_ids, dtype=torch.int64)\n",
        "            parent_ids = torch.as_tensor(parent_ids, dtype=torch.int64)\n",
        "        \n",
        "        image_id_tensor = torch.tensor([image_id])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if num_objs > 0 else torch.zeros((0,))\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "        \n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"masks\": masks,\n",
        "            \"image_id\": image_id_tensor,\n",
        "            \"area\": area,\n",
        "            \"iscrowd\": iscrowd,\n",
        "            \"instance_ids\": instance_ids,\n",
        "            \"parent_ids\": parent_ids\n",
        "        }\n",
        "        \n",
        "        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä\n",
        "        image = torch.as_tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
        "        \n",
        "        if self.transforms:\n",
        "            image, target = self.transforms(image, target)\n",
        "        \n",
        "        return image, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. –ú–æ–¥–µ–ª—å Mask R-CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model(num_classes, pretrained=True):\n",
        "    \"\"\"–°–æ–∑–¥–∞—ë—Ç –º–æ–¥–µ–ª—å Mask R-CNN.\"\"\"\n",
        "    \n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å –Ω–æ–≤—ã–º API\n",
        "    if pretrained:\n",
        "        weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "        model = maskrcnn_resnet50_fpn(weights=weights)\n",
        "    else:\n",
        "        model = maskrcnn_resnet50_fpn(weights=None)\n",
        "    \n",
        "    # –ó–∞–º–µ–Ω—è–µ–º –≥–æ–ª–æ–≤—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    \n",
        "    # –ó–∞–º–µ–Ω—è–µ–º –≥–æ–ª–æ–≤—É –º–∞—Å–æ–∫\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "        in_features_mask, hidden_layer, num_classes\n",
        "    )\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. –§—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.cuda.amp as amp\n",
        "import gc\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"–ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è DataLoader.\"\"\"\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, scaler=None):\n",
        "    \"\"\"–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ —Å AMP.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        \n",
        "        # AMP Context\n",
        "        with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        if scaler is not None:\n",
        "            scaler.scale(losses).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        total_loss += losses.item()\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Epoch [{epoch}] Batch [{batch_idx}/{len(data_loader)}] \"\n",
        "                  f\"Loss: {losses.item():.4f}\")\n",
        "    \n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "    \"\"\"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ (Optimized).\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    \n",
        "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
        "        images = list(img.to(device) for img in images)\n",
        "        \n",
        "        # AMP autocast for inference too to save memory\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(images)\n",
        "        \n",
        "        # Process outputs to save memory\n",
        "        cleaned_outputs = []\n",
        "        for output in outputs:\n",
        "            # Remove masks to save huge amount of RAM - we only need boxes for association metric\n",
        "            # Also keep only top 100\n",
        "            if len(output['scores']) > 100:\n",
        "                keep = torch.argsort(output['scores'], descending=True)[:100]\n",
        "                box = output['boxes'][keep].cpu()\n",
        "                lbl = output['labels'][keep].cpu()\n",
        "                scr = output['scores'][keep].cpu()\n",
        "            else:\n",
        "                box = output['boxes'].cpu()\n",
        "                lbl = output['labels'].cpu()\n",
        "                scr = output['scores'].cpu()\n",
        "            \n",
        "            # Note: We DROP masks here intentionally for global stats\n",
        "            cleaned_outputs.append({'boxes': box, 'labels': lbl, 'scores': scr})\n",
        "            \n",
        "        all_predictions.extend(cleaned_outputs)\n",
        "        all_targets.extend(targets)\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                 torch.cuda.empty_cache()\n",
        "    \n",
        "    return all_predictions, all_targets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–≤—è–∑–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_associations(boxes, labels, scores=None, score_threshold=0.5):\n",
        "    \"\"\"\n",
        "    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–∞–º–∏ –∏ –∫—É–±–∞–º–∏.\n",
        "    \n",
        "    –õ–æ–≥–∏–∫–∞: –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –±–ª–∏–∂–∞–π—à–µ–º—É –∫—É–±—É, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ü–û–î –Ω–∏–º.\n",
        "    \n",
        "    Args:\n",
        "        boxes: (N, 4) bbox –≤ —Ñ–æ—Ä–º–∞—Ç–µ [x1, y1, x2, y2]\n",
        "        labels: (N,) –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤\n",
        "        scores: (N,) –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, confidence scores\n",
        "        score_threshold: –ø–æ—Ä–æ–≥ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n",
        "    \n",
        "    Returns:\n",
        "        dict: {para_idx: cube_idx} - —Å–≤—è–∑–∏\n",
        "    \"\"\"\n",
        "    if isinstance(boxes, torch.Tensor):\n",
        "        boxes = boxes.cpu().numpy()\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.cpu().numpy()\n",
        "    if scores is not None and isinstance(scores, torch.Tensor):\n",
        "        scores = scores.cpu().numpy()\n",
        "    \n",
        "    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –∫—É–±–æ–≤ –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–æ–≤\n",
        "    cube_indices = np.where(labels == 1)[0]\n",
        "    para_indices = np.where(labels == 2)[0]\n",
        "    \n",
        "    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ score –µ—Å–ª–∏ –µ—Å—Ç—å\n",
        "    if scores is not None:\n",
        "        cube_indices = [i for i in cube_indices if scores[i] >= score_threshold]\n",
        "        para_indices = [i for i in para_indices if scores[i] >= score_threshold]\n",
        "    \n",
        "    associations = {}\n",
        "    \n",
        "    for para_idx in para_indices:\n",
        "        para_box = boxes[para_idx]\n",
        "        para_center_x = (para_box[0] + para_box[2]) / 2\n",
        "        para_bottom = para_box[3]  # –Ω–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–∞\n",
        "        \n",
        "        min_dist = float('inf')\n",
        "        best_cube_idx = None\n",
        "        \n",
        "        for cube_idx in cube_indices:\n",
        "            cube_box = boxes[cube_idx]\n",
        "            cube_center_x = (cube_box[0] + cube_box[2]) / 2\n",
        "            cube_top = cube_box[1]  # –≤–µ—Ä—Ö–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ –∫—É–±–∞\n",
        "            \n",
        "            # –ü–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ù–ê–î –∫—É–±–æ–º (–µ–≥–æ –Ω–∏–∑ –±–ª–∏–∑–∫–æ –∫ –≤–µ—Ä—Ö—É –∫—É–±–∞)\n",
        "            # –∏–ª–∏ –ø–µ—Ä–µ—Å–µ–∫–∞—Ç—å—Å—è –ø–æ –≤–µ—Ä—Ç–∏–∫–∞–ª–∏\n",
        "            vertical_dist = abs(para_bottom - cube_top)\n",
        "            horizontal_dist = abs(para_center_x - cube_center_x)\n",
        "            \n",
        "            # –û–±—â–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ\n",
        "            dist = np.sqrt(vertical_dist**2 + horizontal_dist**2)\n",
        "            \n",
        "            # –ë–æ–Ω—É—Å –µ—Å–ª–∏ –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ —Ü–µ–Ω—Ç—Ä—É –∫—É–±–∞\n",
        "            if cube_box[0] <= para_center_x <= cube_box[2]:\n",
        "                dist *= 0.5  # —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ\n",
        "            \n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                best_cube_idx = cube_idx\n",
        "        \n",
        "        associations[int(para_idx)] = int(best_cube_idx) if best_cube_idx is not None else None\n",
        "    \n",
        "    return associations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–≤—è–∑–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_iou(box1, box2):\n",
        "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç IoU –º–µ–∂–¥—É –¥–≤—É–º—è bbox.\"\"\"\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "    \n",
        "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    \n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    \n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "    \n",
        "    return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "\n",
        "def compute_association_accuracy(predictions, targets, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    –í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–≤—è–∑–µ–π.\n",
        "    \n",
        "    –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç,\n",
        "    –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω —Å–≤—è–∑–∞–Ω–Ω—ã–π –∫—É–±.\n",
        "    \"\"\"\n",
        "    correct_associations = 0\n",
        "    total_associations = 0\n",
        "    \n",
        "    for pred, target in zip(predictions, targets):\n",
        "        pred_labels = pred['labels'].cpu().numpy()\n",
        "        pred_boxes = pred['boxes'].cpu().numpy()\n",
        "        pred_scores = pred['scores'].cpu().numpy()\n",
        "        \n",
        "        target_labels = target['labels'].cpu().numpy()\n",
        "        target_boxes = target['boxes'].cpu().numpy()\n",
        "        target_instance_ids = target['instance_ids'].cpu().numpy()\n",
        "        target_parent_ids = target['parent_ids'].cpu().numpy()\n",
        "        \n",
        "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Å–≤—è–∑–∏\n",
        "        pred_associations = predict_associations(pred_boxes, pred_labels, pred_scores)\n",
        "        \n",
        "        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ GT –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥–∞ –Ω–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π\n",
        "        target_para_indices = np.where(target_labels == 2)[0]\n",
        "        pred_para_indices = np.where(pred_labels == 2)[0]\n",
        "        \n",
        "        for target_idx in target_para_indices:\n",
        "            target_box = target_boxes[target_idx]\n",
        "            gt_parent_id = target_parent_ids[target_idx]\n",
        "            \n",
        "            # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π –º–∞—Ç—á —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\n",
        "            best_iou = 0\n",
        "            best_pred_idx = -1\n",
        "            \n",
        "            for pred_idx in pred_para_indices:\n",
        "                if pred_scores[pred_idx] < 0.5:\n",
        "                    continue\n",
        "                pred_box = pred_boxes[pred_idx]\n",
        "                iou = compute_iou(target_box, pred_box)\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_pred_idx = pred_idx\n",
        "            \n",
        "            if best_iou >= iou_threshold and best_pred_idx >= 0:\n",
        "                total_associations += 1\n",
        "                \n",
        "                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å —Å–≤—è–∑–∏\n",
        "                pred_cube_idx = pred_associations.get(best_pred_idx)\n",
        "                \n",
        "                if pred_cube_idx is not None:\n",
        "                    pred_cube_box = pred_boxes[pred_cube_idx]\n",
        "                    \n",
        "                    # –ù–∞—Ö–æ–¥–∏–º GT –∫—É–± —Å —ç—Ç–∏–º parent_id\n",
        "                    gt_cube_indices = np.where(\n",
        "                        (target_labels == 1) & (target_instance_ids == gt_parent_id)\n",
        "                    )[0]\n",
        "                    \n",
        "                    if len(gt_cube_indices) > 0:\n",
        "                        gt_cube_box = target_boxes[gt_cube_indices[0]]\n",
        "                        cube_iou = compute_iou(pred_cube_box, gt_cube_box)\n",
        "                        \n",
        "                        if cube_iou >= iou_threshold:\n",
        "                            correct_associations += 1\n",
        "    \n",
        "    accuracy = correct_associations / total_associations if total_associations > 0 else 0\n",
        "    return accuracy, correct_associations, total_associations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
        "dataset = CubeParallelepipedDataset(\n",
        "    images_path=IMAGES_PATH,\n",
        "    masks_path=MASKS_PATH,\n",
        "    annotations_path=ANNOTATIONS_PATH\n",
        ")\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val\n",
        "train_size = int(TRAIN_RATIO * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ DataLoader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "model = get_model(NUM_CLASSES, pretrained=True)\n",
        "model.to(DEVICE)\n",
        "\n",
        "# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(params, lr=LEARNING_RATE, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "print(f\"Model loaded on {DEVICE}\")\n",
        "\n",
        "# Initialize scaler for AMP\n",
        "scaler = torch.cuda.amp.GradScaler()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # –û–±—É—á–µ–Ω–∏–µ\n",
        "    avg_loss = train_one_epoch(model, optimizer, train_loader, DEVICE, epoch, scaler)\n",
        "    train_losses.append(avg_loss)\n",
        "    \n",
        "    print(f\"\\nAverage training loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ learning rate\n",
        "    lr_scheduler.step()\n",
        "    \n",
        "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f\"Saved best model with loss: {best_loss:.4f}\")\n",
        "    \n",
        "    # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞\n",
        "    # Clear memory before evaluation\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    if epoch % 5 == 0:\n",
        "        predictions, targets = evaluate(model, val_loader, DEVICE)\n",
        "        acc, correct, total = compute_association_accuracy(predictions, targets)\n",
        "        val_accuracies.append((epoch, acc))\n",
        "        print(f\"\\nAssociation accuracy: {acc:.4f} ({correct}/{total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(train_losses, 'b-', label='Training Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Progress')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Association accuracy\n",
        "if val_accuracies:\n",
        "    epochs, accs = zip(*val_accuracies)\n",
        "    axes[1].plot(epochs, accs, 'g-o', label='Association Accuracy')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].set_title('Association Accuracy')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_progress.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(image, prediction, score_threshold=0.5):\n",
        "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å —Å–≤—è–∑—è–º–∏.\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "    \n",
        "    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å bbox\n",
        "    img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
        "    axes[0].imshow(img_np)\n",
        "    axes[0].set_title('Detections')\n",
        "    \n",
        "    labels = prediction['labels'].cpu().numpy()\n",
        "    boxes = prediction['boxes'].cpu().numpy()\n",
        "    scores = prediction['scores'].cpu().numpy()\n",
        "    masks = prediction['masks'].cpu().numpy()\n",
        "    \n",
        "    colors = {1: 'red', 2: 'green'}\n",
        "    \n",
        "    # –†–∏—Å—É–µ–º bbox\n",
        "    for i in range(len(labels)):\n",
        "        if scores[i] >= score_threshold:\n",
        "            box = boxes[i]\n",
        "            label = labels[i]\n",
        "            color = colors.get(label, 'blue')\n",
        "            \n",
        "            rect = plt.Rectangle(\n",
        "                (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
        "                fill=False, edgecolor=color, linewidth=2\n",
        "            )\n",
        "            axes[0].add_patch(rect)\n",
        "            axes[0].text(box[0], box[1] - 5, \n",
        "                        f\"{CATEGORIES[label]}: {scores[i]:.2f}\",\n",
        "                        color=color, fontsize=8)\n",
        "    \n",
        "    # –ú–∞—Å–∫–∏ —Å —Å–≤—è–∑—è–º–∏\n",
        "    combined_mask = np.zeros((*img_np.shape[:2], 3))\n",
        "    \n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–≤—è–∑–∏\n",
        "    associations = predict_associations(boxes, labels, scores, score_threshold)\n",
        "    \n",
        "    for i in range(len(labels)):\n",
        "        if scores[i] >= score_threshold:\n",
        "            mask = masks[i, 0] > 0.5\n",
        "            label = labels[i]\n",
        "            \n",
        "            if label == 1:  # –ö—É–± - –∫—Ä–∞—Å–Ω—ã–π\n",
        "                combined_mask[mask] = [1, 0, 0]\n",
        "            elif label == 2:  # –ü–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥ - –∑–µ–ª—ë–Ω—ã–π\n",
        "                combined_mask[mask] = [0, 1, 0]\n",
        "    \n",
        "    axes[1].imshow(img_np)\n",
        "    axes[1].imshow(combined_mask, alpha=0.5)\n",
        "    axes[1].set_title('Masks with Associations')\n",
        "    \n",
        "    # –†–∏—Å—É–µ–º –ª–∏–Ω–∏–∏ —Å–≤—è–∑–µ–π\n",
        "    for para_idx, cube_idx in associations.items():\n",
        "        if cube_idx is not None:\n",
        "            if scores[para_idx] >= score_threshold and scores[cube_idx] >= score_threshold:\n",
        "                para_box = boxes[para_idx]\n",
        "                cube_box = boxes[cube_idx]\n",
        "                \n",
        "                para_center = [(para_box[0] + para_box[2]) / 2, (para_box[1] + para_box[3]) / 2]\n",
        "                cube_center = [(cube_box[0] + cube_box[2]) / 2, (cube_box[1] + cube_box[3]) / 2]\n",
        "                \n",
        "                axes[1].plot([para_center[0], cube_center[0]], \n",
        "                            [para_center[1], cube_center[1]], \n",
        "                            'y-', linewidth=2)\n",
        "                axes[1].plot(*para_center, 'yo', markersize=8)\n",
        "                axes[1].plot(*cube_center, 'yo', markersize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return associations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ validation set\n",
        "sample_images = []\n",
        "sample_targets = []\n",
        "\n",
        "for i in range(min(5, len(val_dataset))):\n",
        "    img, target = val_dataset[i]\n",
        "    sample_images.append(img)\n",
        "    sample_targets.append(target)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "with torch.no_grad():\n",
        "    images_tensor = [img.to(DEVICE) for img in sample_images]\n",
        "    predictions = model(images_tensor)\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
        "for i, (img, pred, target) in enumerate(zip(sample_images, predictions, sample_targets)):\n",
        "    print(f\"\\n--- Sample {i + 1} ---\")\n",
        "    associations = visualize_predictions(img, pred)\n",
        "    print(f\"Predicted associations: {associations}\")\n",
        "    \n",
        "    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å GT\n",
        "    gt_parent_ids = target['parent_ids'].numpy()\n",
        "    gt_labels = target['labels'].numpy()\n",
        "    gt_instance_ids = target['instance_ids'].numpy()\n",
        "    \n",
        "    print(f\"Ground truth:\")\n",
        "    for j, (label, inst_id, parent_id) in enumerate(zip(gt_labels, gt_instance_ids, gt_parent_ids)):\n",
        "        if label == 2:  # –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥\n",
        "            print(f\"  Para [{inst_id}] -> Cube [{parent_id}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epochs': NUM_EPOCHS,\n",
        "    'train_losses': train_losses,\n",
        "    'val_accuracies': val_accuracies,\n",
        "    'categories': CATEGORIES\n",
        "}, 'cube_parallelepiped_model.pth')\n",
        "\n",
        "print(\"Model saved to cube_parallelepiped_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Inference –Ω–∞ –Ω–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inference(model, image_path, device, score_threshold=0.5):\n",
        "    \"\"\"–ó–∞–ø—É—Å–∫ inference –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.\"\"\"\n",
        "    \n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = torch.as_tensor(np.array(image), dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model([image_tensor.to(device)])[0]\n",
        "    \n",
        "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ score\n",
        "    keep = prediction['scores'] >= score_threshold\n",
        "    filtered_pred = {\n",
        "        'boxes': prediction['boxes'][keep],\n",
        "        'labels': prediction['labels'][keep],\n",
        "        'scores': prediction['scores'][keep],\n",
        "        'masks': prediction['masks'][keep]\n",
        "    }\n",
        "    \n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–≤—è–∑–µ–π\n",
        "    associations = predict_associations(\n",
        "        filtered_pred['boxes'], \n",
        "        filtered_pred['labels'],\n",
        "        filtered_pred['scores']\n",
        "    )\n",
        "    \n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    visualize_predictions(image_tensor, filtered_pred, score_threshold)\n",
        "    \n",
        "    print(f\"\\nDetected objects: {len(filtered_pred['labels'])}\")\n",
        "    print(f\"  Cubes: {(filtered_pred['labels'] == 1).sum().item()}\")\n",
        "    print(f\"  Parallelepipeds: {(filtered_pred['labels'] == 2).sum().item()}\")\n",
        "    print(f\"Associations: {associations}\")\n",
        "    \n",
        "    return filtered_pred, associations\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
        "# result, assoc = inference(model, 'path/to/new/image.png', DEVICE)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "isGpuEnabled": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}