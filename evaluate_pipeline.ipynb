{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üçì Strawberry & Peduncle: Pipeline Evaluation with Depth Anything V3\n",
                "\n",
                "This notebook evaluates the full pipeline using **Metric Depth Estimation**:\n",
                "1.  **Metric Depth**: `Depth Anything V3` estimates depth in meters and camera intrinsics.\n",
                "2.  **Segmentation**: `YOLOv11` detects Strawberries and Peduncles.\n",
                "3.  **Association**: `AffinityNet` matches peduncles to strawberries.\n",
                "4.  **3D Localization**: Combines metric depth with 2D detections to get real-world coordinates.\n",
                "\n",
                "## Models\n",
                "- **Depth**: `Depth-Anything-V3-Large`\n",
                "- **Segmentation**: `yolo11l-seg-strawberry-stem-2`\n",
                "- **Matching**: `affinity-net-strawberry-peduncle-maching-v1`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üì¶ Install Dependencies\n",
                "# Depth Anything V3\n",
                "!pip install git+https://github.com/ByteDance-Seed/Depth-Anything-3.git\n",
                "!pip install xformers torch>=2 torchvision matplotlib opencv-python-headless scikit-learn tqdm ultralytics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import glob\n",
                "import numpy as np\n",
                "import cv2\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "from ultralytics import YOLO\n",
                "from scipy.optimize import linear_sum_assignment\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Import Depth Anything V3\n",
                "try:\n",
                "    from depth_anything_3.api import DepthAnything3\n",
                "    print(\"‚úÖ Depth Anything V3 imported successfully\")\n",
                "except ImportError:\n",
                "    print(\"‚ùå ERROR: depth_anything_3 not installed. Run the pip install cell!\")\n",
                "\n",
                "# === CONFIGURATION ===\n",
                "# Dataset Path check\n",
                "POSSIBLE_DATASET_PATHS = [\n",
                "    \"/kaggle/input/strawberry-peduncle-segmentation/strawberry_peduncle_segmentation/dataset\",\n",
                "    \"/kaggle/input/strawberry-peduncle-segmentation/dataset\",\n",
                "    \"dataset\" # Local fallback\n",
                "]\n",
                "DATASET_PATH = None\n",
                "for p in POSSIBLE_DATASET_PATHS:\n",
                "    if os.path.exists(p):\n",
                "        DATASET_PATH = p\n",
                "        break\n",
                "\n",
                "if DATASET_PATH is None:\n",
                "    print(\"‚ö†Ô∏è DATASET NOT FOUND! Please check input paths.\")\n",
                "    DATASET_PATH = \"dataset\"\n",
                "\n",
                "print(f\"üìÇ using Dataset Path: {DATASET_PATH}\")\n",
                "\n",
                "# Model Weights Check\n",
                "WEIGHTS_YOLO = \"/kaggle/input/yolo11l-seg-strawberry-stem-2/pytorch/default/1/yolo11l-seg-strawberry-stem-2.pt\"\n",
                "WEIGHTS_AFFINITY = \"/kaggle/input/affinity-net-strawberry-peduncle-maching-v1/pytorch/default/1/best_affinity_net.pth\"\n",
                "\n",
                "# Check if weights exist, simple search if not\n",
                "if not os.path.exists(WEIGHTS_YOLO):\n",
                "    found = glob.glob(\"/kaggle/input/yolo11l-seg-strawberry-stem-2/**/*.pt\", recursive=True)\n",
                "    if found:\n",
                "        WEIGHTS_YOLO = found[0]\n",
                "\n",
                "if not os.path.exists(WEIGHTS_AFFINITY):\n",
                "    found = glob.glob(\"/kaggle/input/affinity-net-strawberry-peduncle-maching-v1/**/*.pth\", recursive=True)\n",
                "    if found:\n",
                "        WEIGHTS_AFFINITY = found[0]\n",
                "\n",
                "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"‚úÖ Device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Define AffinityNet & Utils"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AffinityNet(nn.Module):\n",
                "    def __init__(self, spatial_dim=5, hidden_dims=[32, 16]):\n",
                "        super().__init__()\n",
                "        layers = []\n",
                "        prev_dim = spatial_dim\n",
                "        for hidden_dim in hidden_dims:\n",
                "            layers.extend([\n",
                "                nn.Linear(prev_dim, hidden_dim),\n",
                "                nn.ReLU(inplace=True),\n",
                "                nn.Dropout(0.3)\n",
                "            ])\n",
                "            prev_dim = hidden_dim\n",
                "        layers.extend([\n",
                "            nn.Linear(prev_dim, 1),\n",
                "            nn.Sigmoid()\n",
                "        ])\n",
                "        self.network = nn.Sequential(*layers)\n",
                "\n",
                "    def forward(self, spatial_features):\n",
                "        return self.network(spatial_features)\n",
                "\n",
                "    def predict_matrix(self, spatial_matrix):\n",
                "        N_para, N_cube, _ = spatial_matrix.shape\n",
                "        spatial_flat = spatial_matrix.reshape(-1, 5)\n",
                "        affinity_flat = self.forward(spatial_flat)\n",
                "        return affinity_flat.reshape(N_para, N_cube)\n",
                "\n",
                "def compute_spatial_features_batch(para_bboxes, cube_bboxes, para_masks, cube_masks, image_size):\n",
                "    H, W = image_size\n",
                "    N_para = len(para_bboxes)\n",
                "    N_cube = len(cube_bboxes)\n",
                "    spatial_matrix = np.zeros((N_para, N_cube, 5), dtype=np.float32)\n",
                "    \n",
                "    for i in range(N_para):\n",
                "        for j in range(N_cube):\n",
                "            # Extract features (simplified for brevity, ensuring complete notebook flow)\n",
                "            px1, py1, px2, py2 = para_bboxes[i]\n",
                "            cx1, cy1, cx2, cy2 = cube_bboxes[j]\n",
                "            \n",
                "            vertical_dist = abs(py2 - cy1) / H\n",
                "            vertical_score = max(0, 1.0 - vertical_dist * 5.0)\n",
                "            \n",
                "            overlap_left = max(px1, cx1)\n",
                "            overlap_right = min(px2, cx2)\n",
                "            overlap_width = max(0, overlap_right - overlap_left)\n",
                "            para_width = px2 - px1\n",
                "            horizontal_overlap = overlap_width / (para_width + 1e-6)\n",
                "            \n",
                "            para_center_x = (px1 + px2) / 2\n",
                "            cube_center_x = (cx1 + cx2) / 2\n",
                "            cube_width = cx2 - cx1\n",
                "            offset = abs(para_center_x - cube_center_x)\n",
                "            centeredness = max(0, 1.0 - offset / (cube_width / 2 + 1e-6))\n",
                "            \n",
                "            para_area = (px2 - px1) * (py2 - py1)\n",
                "            cube_area = (cx2 - cx1) * (cy2 - cy1)\n",
                "            size_ratio = min(para_area / (cube_area + 1e-6), 1.0)\n",
                "            \n",
                "            mask_iou = 0.0\n",
                "            if para_masks is not None and cube_masks is not None:\n",
                "                pm = para_masks[i]\n",
                "                cm = cube_masks[j]\n",
                "                intersection = np.logical_and(pm, cm).sum()\n",
                "                union = np.logical_or(pm, cm).sum()\n",
                "                mask_iou = intersection / (union + 1e-6)\n",
                "                \n",
                "            spatial_matrix[i, j] = [vertical_score, horizontal_overlap, centeredness, size_ratio, mask_iou]\n",
                "            \n",
                "    return spatial_matrix"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. YOLO\n",
                "print(\"üöÄ Loading YOLO...\")\n",
                "try:\n",
                "    if os.path.exists(WEIGHTS_YOLO):\n",
                "        yolo_model = YOLO(WEIGHTS_YOLO)\n",
                "        print(\"‚úÖ YOLO Loaded\")\n",
                "    else:\n",
                "        print(f\"‚ùå ERROR: YOLO Weights not found!\")\n",
                "        yolo_model = None\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è Error loading YOLO: {e}\")\n",
                "    yolo_model = None\n",
                "\n",
                "# 2. AffinityNet\n",
                "print(\"üöÄ Loading AffinityNet...\")\n",
                "affinity_model = AffinityNet().to(DEVICE)\n",
                "if os.path.exists(WEIGHTS_AFFINITY):\n",
                "    affinity_model.load_state_dict(torch.load(WEIGHTS_AFFINITY, map_location=DEVICE))\n",
                "    affinity_model.eval()\n",
                "    print(\"‚úÖ AffinityNet Loaded\")\n",
                "else:\n",
                "    print(f\"‚ùå ERROR: AffinityNet Weights not found!\")\n",
                "    \n",
                "# 3. Depth Anything V3\n",
                "print(\"üöÄ Loading Depth Anything V3 (Metric)...\")\n",
                "try:\n",
                "    DEPTH_MODEL_NAME = \"Depth-Anything-V3-Large\" # Or \"Depth-Anything-V3-Small\"\n",
                "    depth_model = DepthAnything3.from_pretrained(DEPTH_MODEL_NAME).to(DEVICE)\n",
                "    print(\"‚úÖ Depth Anything V3 Loaded (with Intrinsics estimation)\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error loading Depth Anything V3: {e}\")\n",
                "    depth_model = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Processing Pipeline\n",
                "\n",
                "We use the depth map from DA-V3 to get the Z-coordinate (in meters) for each object.\n",
                "We use the **inferred intrinsics** from DA-V3 to calculate X and Y."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pixel_to_3d_metric(bbox, depth_map, intrinsics):\n",
                "    \"\"\"\n",
                "    Calculates 3D position using the metric depth map and intrinsics.\n",
                "    \"\"\"\n",
                "    fx = intrinsics[0, 0]\n",
                "    fy = intrinsics[1, 1]\n",
                "    cx = intrinsics[0, 2]\n",
                "    cy = intrinsics[1, 2]\n",
                "    \n",
                "    x1, y1, x2, y2 = map(int, bbox)\n",
                "    # Ensure coords are within image bounds\n",
                "    H, W = depth_map.shape\n",
                "    x1, x2 = max(0, x1), min(W, x2)\n",
                "    y1, y2 = max(0, y1), min(H, y2)\n",
                "    \n",
                "    if x2 <= x1 or y2 <= y1:\n",
                "        return 0.0, 0.0, 0.0\n",
                "        \n",
                "    # Get median depth in the bounding box region\n",
                "    # (Can be refined to use mask if available)\n",
                "    depth_crop = depth_map[y1:y2, x1:x2]\n",
                "    Z = np.median(depth_crop)\n",
                "    \n",
                "    # Centroid\n",
                "    u = (x1 + x2) / 2\n",
                "    v = (y1 + y2) / 2\n",
                "    \n",
                "    X = (u - cx) * Z / fx\n",
                "    Y = -(v - cy) * Z / fy # Invert Y for standard coordinate systems if needed\n",
                "    \n",
                "    return float(X), float(Y), float(Z)\n",
                "\n",
                "def process_image(image_path, visualize=False):\n",
                "    if yolo_model is None or depth_model is None:\n",
                "        print(\"‚ùå Models not loaded correctly.\")\n",
                "        return None\n",
                "    \n",
                "    img_filename = os.path.basename(image_path)\n",
                "    \n",
                "    # 1. Load Image\n",
                "    image_cv = cv2.imread(image_path)\n",
                "    image_rgb = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)\n",
                "    image_pil = Image.fromarray(image_rgb)\n",
                "    H, W, _ = image_rgb.shape\n",
                "\n",
                "    # 2. Metric Depth Inference\n",
                "    with torch.no_grad():\n",
                "        depth_pred = depth_model.inference([image_pil])\n",
                "        \n",
                "    depth_map = depth_pred.depth[0] # (H, W) in meters\n",
                "    intrinsics = depth_pred.intrinsics[0].cpu().numpy() # (3, 3)\n",
                "    \n",
                "    # 3. YOLO Inference\n",
                "    results = yolo_model(image_path, conf=0.5, verbose=False)[0]\n",
                "    boxes = results.boxes.xyxy.cpu().numpy()\n",
                "    classes = results.boxes.cls.cpu().numpy()\n",
                "    masks = results.masks.data.cpu().numpy() if results.masks else None\n",
                "    \n",
                "    cube_indices = np.where(classes == 0)[0]\n",
                "    para_indices = np.where(classes == 1)[0]\n",
                "    \n",
                "    cubes = []\n",
                "    parallelepipeds = []\n",
                "    \n",
                "    # Process Objects\n",
                "    for idx in cube_indices:\n",
                "        box = boxes[idx]\n",
                "        x, y, z = pixel_to_3d_metric(box, depth_map, intrinsics)\n",
                "        cubes.append({\n",
                "            'id': int(idx),\n",
                "            'bbox': box.tolist(),\n",
                "            'mask': masks[idx] if masks is not None else None,\n",
                "            'pos_3d': [x, y, z],\n",
                "            'class': 'strawberry'\n",
                "        })\n",
                "        \n",
                "    for idx in para_indices:\n",
                "        box = boxes[idx]\n",
                "        x, y, z = pixel_to_3d_metric(box, depth_map, intrinsics)\n",
                "        parallelepipeds.append({\n",
                "            'id': int(idx),\n",
                "            'bbox': box.tolist(),\n",
                "            'mask': masks[idx] if masks is not None else None,\n",
                "            'pos_3d': [x, y, z],\n",
                "            'class': 'peduncle',\n",
                "            'matched_cube_id': None\n",
                "        })\n",
                "        \n",
                "    # 4. Association\n",
                "    if len(cubes) > 0 and len(parallelepipeds) > 0:\n",
                "        cube_boxes = np.array([c['bbox'] for c in cubes])\n",
                "        para_boxes = np.array([p['bbox'] for p in parallelepipeds])\n",
                "        cm = np.array([c['mask'] for c in cubes]) if masks is not None else None\n",
                "        pm = np.array([p['mask'] for p in parallelepipeds]) if masks is not None else None\n",
                "        \n",
                "        spatial_matrix = compute_spatial_features_batch(para_boxes, cube_boxes, pm, cm, (H, W))\n",
                "        spatial_t = torch.from_numpy(spatial_matrix).to(DEVICE)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            affinity = affinity_model.predict_matrix(spatial_t).cpu().numpy()\n",
                "            \n",
                "        row_ind, col_ind = linear_sum_assignment(-affinity)\n",
                "        for r, c in zip(row_ind, col_ind):\n",
                "            if affinity[r, c] > 0.5:\n",
                "                parallelepipeds[r]['matched_cube_id'] = cubes[c]['id']\n",
                "\n",
                "    # 5. Visualization\n",
                "    if visualize:\n",
                "        plt.figure(figsize=(15, 6))\n",
                "        \n",
                "        # RGB + Bounding Boxes\n",
                "        plt.subplot(1, 2, 1)\n",
                "        vis_img = image_rgb.copy()\n",
                "        for c in cubes:\n",
                "            x1, y1, x2, y2 = map(int, c['bbox'])\n",
                "            cv2.rectangle(vis_img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
                "            cv2.putText(vis_img, f\"{c['pos_3d'][2]:.2f}m\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
                "            \n",
                "        for p in parallelepipeds:\n",
                "            x1, y1, x2, y2 = map(int, p['bbox'])\n",
                "            cv2.rectangle(vis_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
                "            cv2.putText(vis_img, f\"{p['pos_3d'][2]:.2f}m\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
                "        \n",
                "        plt.imshow(vis_img)\n",
                "        plt.title(f\"Detections & Metric Depth Labels\")\n",
                "        plt.axis('off')\n",
                "        \n",
                "        # Depth Map\n",
                "        plt.subplot(1, 2, 2)\n",
                "        plt.imshow(depth_map, cmap='turbo')\n",
                "        plt.colorbar(label='Depth (m)')\n",
                "        plt.title(f\"Depth Anything V3 Output\")\n",
                "        plt.axis('off')\n",
                "        plt.show()\n",
                "        \n",
                "    # Output Format\n",
                "    output = {'image': img_filename, 'strawberries': [], 'peduncles': []}\n",
                "    for c in cubes:\n",
                "        d = {k:v for k,v in c.items() if k!='mask'}\n",
                "        output['strawberries'].append(d)\n",
                "    for p in parallelepipeds:\n",
                "        d = {k:v for k,v in p.items() if k!='mask'}\n",
                "        output['peduncles'].append(d)\n",
                "        \n",
                "    return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === RUN ON TEST IMAGES ===\n",
                "if DATASET_PATH is not None:\n",
                "    search_paths = [\n",
                "        os.path.join(DATASET_PATH, \"images\", \"*.png\"),\n",
                "        os.path.join(DATASET_PATH, \"*.png\")\n",
                "    ]\n",
                "    test_images = []\n",
                "    for sp in search_paths:\n",
                "        found = glob.glob(sp)\n",
                "        if len(found) > 0:\n",
                "            test_images = found\n",
                "            break\n",
                "    test_images = test_images[:5]\n",
                "else:\n",
                "    test_images = []\n",
                "\n",
                "print(f\"üß™ Testing on {len(test_images)} images...\")\n",
                "\n",
                "results_json = []\n",
                "for img_path in test_images:\n",
                "    try:\n",
                "        res = process_image(img_path, visualize=True)\n",
                "        if res:\n",
                "            results_json.append(res)\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Error processing {img_path}: {e}\")\n",
                "\n",
                "# Save\n",
                "with open(\"detailed_results.json\", \"w\") as f:\n",
                "    json.dump(results_json, f, indent=2)\n",
                "print(\"‚úÖ Saved detailed_results.json\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 3D PLOT ===\n",
                "def plot_3d_topdown(results):\n",
                "    plt.figure(figsize=(10, 10))\n",
                "    for entry in results:\n",
                "        for s in entry['strawberries']:\n",
                "            x, y, z = s['pos_3d']\n",
                "            plt.scatter(x, z, c='red', marker='s', s=100, label='Strawberry' if 'Strawberry' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
                "        for p in entry['peduncles']:\n",
                "            x, y, z = p['pos_3d']\n",
                "            plt.scatter(x, z, c='green', marker='^', s=50, label='Peduncle' if 'Peduncle' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
                "            if p['matched_cube_id']:\n",
                "                match = next((s for s in entry['strawberries'] if s['id'] == p['matched_cube_id']), None)\n",
                "                if match:\n",
                "                    mx, my, mz = match['pos_3d']\n",
                "                    plt.plot([x, mx], [z, mz], 'k--', alpha=0.3)\n",
                "    plt.xlabel(\"X (meters)\")\n",
                "    plt.ylabel(\"Z (Depth, meters)\")\n",
                "    plt.title(\"Top-Down View of Detected Objects (Metric Depth)\")\n",
                "    plt.grid(True)\n",
                "    plt.legend()\n",
                "    plt.axis('equal')\n",
                "    plt.show()\n",
                "\n",
                "if results_json:\n",
                "    plot_3d_topdown(results_json)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}